{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efc8134d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n",
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "%matplotlib notebook\n",
    "from IPython.display import HTML\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "#Import usual things required for graph nets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import sonnet as snt\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "import logging, os\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from matplotlib.patches import Polygon\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Import graph nets\n",
    "from graph_nets import blocks\n",
    "from graph_nets import utils_tf\n",
    "from graph_nets import utils_np\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "#Set seed\n",
    "SEED = 4038\n",
    "\n",
    "np.random.seed(SEED)\n",
    "rand = np.random.RandomState(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4de0733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current_date: 2021-05-01 23:24:16.120727\n"
     ]
    }
   ],
   "source": [
    "#Path to the scripts shared troughout phases\n",
    "parent_path=os.path.abspath('..\\\\..')\n",
    "path_common =  '\\\\'.join(parent_path.split('\\\\')[:-1]) \n",
    "\n",
    "#Check if path exists\n",
    "path_common_scripts = os.path.join(path_common,'Common','Scripts')\n",
    "if(not os.path.exists(path_common_scripts)):\n",
    "    print('The path: '+ path_common_scripts+' is not found!')\n",
    "path_common = os.path.join(path_common,'Common')\n",
    "#Check if path exists\n",
    "if(not os.path.exists(path_common)):\n",
    "    print('The path: '+ path_common+' is not found!')\n",
    "\n",
    "\n",
    "\n",
    "#Add shared scripts to our current paths for importing\n",
    "sys.path.insert(0, path_common_scripts)\n",
    "\n",
    "#Retrieve test specification document\n",
    "test_spec_json = os.path.join(path_common,'hyper_parameters.json')\n",
    "#Check if file exists\n",
    "if(not os.path.exists(test_spec_json)):\n",
    "    print('The file: '+ test_spec_json+' is not found!')\n",
    "with open(test_spec_json) as f:\n",
    "    test_spec = json.load(f)\n",
    "    \n",
    "params = test_spec['parameters']\n",
    "\n",
    "model_params = params['model_parameters']\n",
    "\n",
    "experiment_params = params['experiment_parameters']\n",
    "\n",
    "#Save experiment begin date\n",
    "current_time = datetime.datetime.now()\n",
    "print(\"Current_date: \"+str(current_time))\n",
    "current_time = \"\".join(current_time.strftime(\"%x\").split('/'))\n",
    "\n",
    "NUM_TRAINING_ITERATIONS = experiment_params['number_of_training_iterations']\n",
    "LOSS_TYPE = experiment_params['loss_type']\n",
    "#BATCH_SIZE_TR = experiment_params['training_batch_size']\n",
    "#BATCH_SIZE_TE = experiment_params['testing_batch_size']\n",
    "#BATCH_SIZE_GE = experiment_params['generalisation_batch_size']\n",
    "NUM_PROCESSING_STEPS_TR = experiment_params['number_of_processing_steps_tr']\n",
    "NUM_PROCESSING_STEPS_TE = experiment_params['number_of_processing_steps_te']\n",
    "NUM_PROCESSING_STEPS_GE = experiment_params['number_of_processing_steps_ge']\n",
    "#ROLLOUT_TIMESTEPS = experiment_params['rollout_timesteps']\n",
    "#ROLLOUT_TIMESTEPS_VIS = experiment_params['rollout_timesteps_vis']\n",
    "\n",
    "FUL_LATENT_SIZE = model_params['ful_block_latent_size']\n",
    "FUL_LAYER_NUMBER = model_params['ful_block_number_of_layers']\n",
    "IND_LATENT_SIZE = model_params['ind_block_latent_size']\n",
    "IND_LAYER_NUMBER = model_params['ind_block_number_of_layers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5ffcbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model\n",
    "import GNNModels_tf2 as models\n",
    "# Import training plot\n",
    "from TrainingPlot import training_plot, training_plot_log_scale\n",
    "# Import rollout plot\n",
    "from RolloutPlot import rollout_plot, rollout_plot_log_scale\n",
    "# Import functions for creating graph\n",
    "from Graph_creator_functions import rigid_graph_from_pos_all, rigid_graph_from_pos_closest\n",
    "# Import data loader module\n",
    "from PushDatasetLoad import load_dataset, collect_trajectory_indeces, create_train_valid_test, collect_states, remove_effector, outlier_remover\n",
    "# Import Dataset Feeder function\n",
    "from PushDatasetFeed import DataFeeder\n",
    "# Import simulation functions\n",
    "from PushDatasetSimulator import PushDatasetSimulator\n",
    "# Import function for experiments\n",
    "from LossFunctions import position_loss_single_step, rollout_loss_mean_position, rollout_error_velocity, rollout_error_position\n",
    "# Import visualisation functions\n",
    "from VisualisationFunctions import visualise_trajectory, visualise_groundtruth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82fa2a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path to Datasets\n",
    "path_dataset = os.path.abspath('..\\\\..\\\\..\\\\..\\\\..')\n",
    "path_dataset = os.path.join(path_dataset,'Data')\n",
    "\n",
    "if(not os.path.exists(path_dataset)):\n",
    "    print(\"Data folder does not exist.\")\n",
    "    \n",
    "# Set paths:\n",
    "file_1 = os.path.join(path_dataset, \"Rect1_Data10ms_1_of_2_flagged.csv\")\n",
    "file_2 = os.path.join(path_dataset, \"Rect1_Data10ms_2_of_2_flagged.csv\")\n",
    "# Load dataset\n",
    "df_set_1, STEP_SIZE = load_dataset(file_1)\n",
    "df_set_2, STEP_SIZE = load_dataset(file_2)\n",
    "\n",
    "# Collect trajectory indeces\n",
    "VEL_ACCS = ['10.0-0.0',\n",
    "            '100.0-0.0', \n",
    "            '150.0-0.0',\n",
    "            '20.0-0.0',\n",
    "            '200.0-0.0',\n",
    "            '300.0-0.0', \n",
    "            '400.0-0.0', \n",
    "            '50.0-0.0', \n",
    "            '500.0-0.0', \n",
    "            '75.0-0.0']\n",
    "trajectory_indeces,df_set_1 = collect_trajectory_indeces(df_set_1, ts = STEP_SIZE, vel_accs = VEL_ACCS)\n",
    "\n",
    "_, df_set_2 = collect_trajectory_indeces(df_set_2, ts = STEP_SIZE, vel_accs = VEL_ACCS)\n",
    "# Split indeces so that no trajectory is contained in both train, validation or test datasets\n",
    "tr_inds, va_inds, te_inds = create_train_valid_test(trajectory_indeces, 0.20)\n",
    "\n",
    "# Collect trajectories from these \n",
    "df_tr_1 = collect_states(tr_inds, df_set_1)\n",
    "df_tr_2 = collect_states(tr_inds, df_set_2)\n",
    "df_va_1 = collect_states(va_inds, df_set_1)\n",
    "df_va_2 = collect_states(va_inds, df_set_2)\n",
    "df_te_1 = collect_states(te_inds, df_set_1)\n",
    "df_te_2 = collect_states(te_inds, df_set_2)\n",
    "\n",
    "# Remove Outliers\n",
    "STATIONARY_THRESHOLD =  0.0001\n",
    "df_tr_1, df_tr_2 = outlier_remover(df_tr_1, df_tr_2, STATIONARY_THRESHOLD)\n",
    "df_va_1, df_va_2 = outlier_remover(df_va_1, df_va_2, STATIONARY_THRESHOLD)\n",
    "df_te_1, df_te_2 = outlier_remover(df_te_1, df_te_2, STATIONARY_THRESHOLD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1f75994",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trajectory\n",
       "4.0       50\n",
       "10.0      50\n",
       "25.0      28\n",
       "39.0       4\n",
       "43.0      49\n",
       "          ..\n",
       "3913.0    56\n",
       "3924.0    67\n",
       "3944.0    25\n",
       "3949.0    47\n",
       "3952.0    11\n",
       "Name: id, Length: 348, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df = df_te_1.groupby('trajectory')['id'].nunique()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f928d855",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_formating(df_1, df_2, roll = None):\n",
    "    df_1a = df_1.copy()\n",
    "    df_2a = df_2.copy()\n",
    "    \n",
    "    saa = df_1a.groupby([\"trajectory\"])[[\"id\"]].count()\n",
    "    \n",
    "    saa[\"traj\"] = saa.index.values.tolist()\n",
    "    cols = saa.columns.tolist()\n",
    "    cols = cols[-1:] + cols[:-1]\n",
    "    saa = saa[cols]\n",
    "    saa_np = saa.to_numpy()\n",
    "    \n",
    "    if roll is None:\n",
    "        roll = np.mean(saa_np,axis=0)[1]\n",
    "    \n",
    "    max_rollout_length = np.min(saa_np[saa_np[:,1] >= roll],axis=0)[1]\n",
    "    max_batch_size = len(saa_np[saa_np[:,1] >= roll])\n",
    "    \n",
    "    traj_inds = saa_np[saa_np[:,1] >= roll][:,0]\n",
    "    \n",
    "    df_1a.set_index('trajectory', drop=False, inplace = True)\n",
    "    df_2a.set_index('trajectory', drop=False, inplace = True)\n",
    "    \n",
    "    df_1a = df_1a.loc[traj_inds]\n",
    "    df_2a = df_2a.loc[traj_inds]\n",
    "    \n",
    "    df_1a.reset_index(drop=True, inplace = True)\n",
    "    df_2a.reset_index(drop=True, inplace = True)\n",
    "    \n",
    "    print(max_rollout_length)\n",
    "    return max_rollout_length, max_batch_size, df_1a, df_2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f46160b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.0\n"
     ]
    }
   ],
   "source": [
    "max_rollout_length, max_batch_size, df_te_1, df_te_2 = dataset_formating(df_te_1, df_te_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bb5c33a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trajectory\n",
       "4.0       50\n",
       "10.0      50\n",
       "43.0      49\n",
       "54.0      53\n",
       "87.0      47\n",
       "          ..\n",
       "3904.0    61\n",
       "3906.0    68\n",
       "3913.0    56\n",
       "3924.0    67\n",
       "3949.0    47\n",
       "Name: id, Length: 108, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df = df_te_1.groupby('trajectory')['id'].nunique()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4472684",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70abbb4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
