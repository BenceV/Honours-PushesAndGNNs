{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n",
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "%matplotlib notebook\n",
    "from IPython.display import HTML\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "#Import usual things required for graph nets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import sonnet as snt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from matplotlib.patches import Polygon\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Import graph nets\n",
    "from graph_nets import blocks\n",
    "from graph_nets import utils_tf\n",
    "from graph_nets import utils_np\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current_date: 2021-04-08 16:09:27.080443\n"
     ]
    }
   ],
   "source": [
    "#Path to the scripts shared troughout phases\n",
    "parent_path=os.path.abspath('..')\n",
    "path_common =  '\\\\'.join(parent_path.split('\\\\')[:-2]) \n",
    "\n",
    "#Check if path exists\n",
    "path_common_scripts = os.path.join(path_common,'Common','Scripts')\n",
    "if(not os.path.exists(path_common_scripts)):\n",
    "    print('The path: '+ path_common_scripts+' is not found!')\n",
    "path_common = os.path.join(path_common,'Common')\n",
    "#Check if path exists\n",
    "if(not os.path.exists(path_common)):\n",
    "    print('The path: '+ path_common+' is not found!')\n",
    "\n",
    "\n",
    "\n",
    "#Add shared scripts to our current paths for importing\n",
    "sys.path.insert(0, path_common_scripts)\n",
    "\n",
    "#Retrieve test specification document\n",
    "test_spec_json = os.path.join(path_common,'hyper_parameters.json')\n",
    "#Check if file exists\n",
    "if(not os.path.exists(test_spec_json)):\n",
    "    print('The file: '+ test_spec_json+' is not found!')\n",
    "with open(test_spec_json) as f:\n",
    "    test_spec = json.load(f)\n",
    "    \n",
    "params = test_spec['parameters']\n",
    "\n",
    "model_params = params['model_parameters']\n",
    "\n",
    "experiment_params = params['experiment_parameters']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Save experiment begin date\n",
    "current_time = datetime.datetime.now()\n",
    "print(\"Current_date: \"+str(current_time))\n",
    "current_time = \"\".join(current_time.strftime(\"%x\").split('/'))\n",
    "\n",
    "NUM_TRAINING_ITERATIONS = experiment_params['number_of_training_iterations']\n",
    "BATCH_SIZE_TR = experiment_params['training_batch_size']\n",
    "BATCH_SIZE_TE = experiment_params['testing_batch_size']\n",
    "BATCH_SIZE_GE = experiment_params['generalisation_batch_size']\n",
    "NUM_PROCESSING_STEPS_TR = experiment_params['number_of_processing_steps_tr']\n",
    "NUM_PROCESSING_STEPS_TE = experiment_params['number_of_processing_steps_te']\n",
    "NUM_PROCESSING_STEPS_GE = experiment_params['number_of_processing_steps_ge']\n",
    "\n",
    "FUL_LATENT_SIZE = model_params['ful_block_latent_size']\n",
    "FUL_LAYER_NUMBER = model_params['ful_block_number_of_layers']\n",
    "IND_LATENT_SIZE = model_params['ind_block_latent_size']\n",
    "IND_LAYER_NUMBER = model_params['ind_block_number_of_layers']\n",
    "\n",
    "\n",
    "# Folder description to distinguish between experiment runs\n",
    "desc = \"ntr=\"+str(NUM_TRAINING_ITERATIONS)\n",
    "desc = desc +\"_btr=\"+str(BATCH_SIZE_TR)\n",
    "desc = desc +\"_high\"\n",
    "desc = desc +\"_date=\"+str(current_time)\n",
    "\n",
    "#Set seed\n",
    "SEED = 4893\n",
    "desc = desc +\"seed=\"+str(SEED)\n",
    "\n",
    "np.random.seed(SEED)\n",
    "rand = np.random.RandomState(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = os.getcwd()\n",
    "if(not os.path.exists(base)):\n",
    "    os.makedirs(base)\n",
    "    \n",
    "#Path to saved models and trajectories\n",
    "path_saves = os.path.join(base,desc,'Saves')\n",
    "if(not os.path.exists(path_saves)):\n",
    "    os.makedirs(path_saves)\n",
    "    \n",
    "#Path to plots\n",
    "path_plots = os.path.join(base,desc,'Plots')\n",
    "if(not os.path.exists(path_plots)):\n",
    "    os.makedirs(path_plots)\n",
    "    \n",
    "#Path to animations\n",
    "path_animations = os.path.join(base,desc,'Animations')\n",
    "if(not os.path.exists(path_animations)):\n",
    "    os.makedirs(path_animations)\n",
    "    \n",
    "#Save the used parameters as a json file into this folder\n",
    "with open(os.path.join(path_saves,'specification.json'), 'w') as fp:\n",
    "    json.dump(test_spec, fp)\n",
    "    \n",
    "#Path to Datasets\n",
    "path_dataset = os.path.abspath('..\\\\..\\\\..\\\\..\\\\..')\n",
    "path_dataset = os.path.join(path_dataset,'Data')\n",
    "\n",
    "if(not os.path.exists(path_dataset)):\n",
    "    print(\"Data folder does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model\n",
    "import GNNModels_tf2 as models\n",
    "# Import training plot\n",
    "from TrainingPlot import training_plot, training_plot_log_scale\n",
    "# Import functions for creating graph\n",
    "from Graph_creator_functions import rigid_graph_from_pos_all, rigid_graph_from_pos_closest\n",
    "# Import data loader module\n",
    "from PushDatasetLoad import load_dataset, collect_trajectory_indeces, create_train_valid_test, collect_states, remove_effector, outlier_remover\n",
    "# Import Dataset Feeder function\n",
    "from PushDatasetFeed import DataFeeder\n",
    "# Import simulation functions\n",
    "from PushDatasetSimulator import PushDatasetSimulator\n",
    "# Import function for experiments\n",
    "from LossFunctions import velocity_loss_single_step, velocity_loss_single_step_high, make_all_runnable_in_session\n",
    "# Import visualisation functions\n",
    "from VisualisationFunctions import visualise_trajectory, visualise_groundtruth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading:\n",
    "In this section of the code we load the data. Split the trajectories into training, validation and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['100.0-0.0']\n",
      "['100.0-0.0']\n",
      "['100.0-0.0']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vbenc\\miniconda3\\envs\\Honours\\lib\\site-packages\\pandas\\core\\frame.py:3191: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[k1] = value[k2]\n",
      "C:\\Users\\vbenc\\miniconda3\\envs\\Honours\\lib\\site-packages\\pandas\\core\\indexing.py:1720: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n"
     ]
    }
   ],
   "source": [
    "# Set paths:\n",
    "file_1 = os.path.join(path_dataset, \"Rect1_Data100ms_1_of_2_flagged.csv\")\n",
    "file_2 = os.path.join(path_dataset, \"Rect1_Data100ms_2_of_2_flagged.csv\")\n",
    "# Load dataset\n",
    "df_set_1, STEP_SIZE = load_dataset(file_1)\n",
    "df_set_2, STEP_SIZE = load_dataset(file_2)\n",
    "# Collect trajectory indeces\n",
    "VEL_ACCS = ['100.0-0.0']\n",
    "trajectory_indeces,df_set_1 = collect_trajectory_indeces(df_set_1, ts = STEP_SIZE, vel_accs = VEL_ACCS)\n",
    "_, df_set_2 = collect_trajectory_indeces(df_set_2, ts = STEP_SIZE, vel_accs = VEL_ACCS)\n",
    "# Split indeces so that no trajectory is contained in both train, validation or test datasets\n",
    "tr_inds, va_inds, te_inds = create_train_valid_test(trajectory_indeces, 0.25)\n",
    "# Collect trajectories from these \n",
    "df_tr_1 = collect_states(tr_inds, df_set_1)\n",
    "df_tr_2 = collect_states(tr_inds, df_set_2)\n",
    "df_va_1 = collect_states(va_inds, df_set_1)\n",
    "df_va_2 = collect_states(va_inds, df_set_2)\n",
    "df_te_1 = collect_states(te_inds, df_set_1)\n",
    "df_te_2 = collect_states(te_inds, df_set_2)\n",
    "\n",
    "# Remove Outliers\n",
    "df_tr_1, df_tr_2 = outlier_remover(df_tr_1, df_tr_2)\n",
    "df_va_1, df_va_2 = outlier_remover(df_va_1, df_va_2)\n",
    "df_te_1, df_te_2 = outlier_remover(df_te_1, df_te_2)\n",
    "\n",
    "\n",
    "df_dict = {\"tr_1\":df_tr_1, \n",
    "           \"tr_2\":df_tr_2, \n",
    "           \"va_1\":df_va_1,\n",
    "           \"va_2\":df_va_2,\n",
    "           \"te_1\":df_te_1,\n",
    "           \"te_2\":df_te_2,}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model.\n",
    "model = models.EncodeProcessDecode(ind_layer_numbers = IND_LAYER_NUMBER,\n",
    "                                   ind_latent_unit_size = IND_LATENT_SIZE,\n",
    "                                   ful_layer_numbers = FUL_LAYER_NUMBER,\n",
    "                                   ful_latent_unit_size = FUL_LATENT_SIZE,\n",
    "                                   node_output_size=2,\n",
    "                                   edge_output_size=2,\n",
    "                                   global_output_size=1)\n",
    "\n",
    "\n",
    "\n",
    "LEARNING_RATE = model_params['learning_rate']\n",
    "#optimizer = tf.keras.optimizers.Adam()\n",
    "optimizer = snt.optimizers.Adam(LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_step(inputs_tr, targets_tr):\n",
    "    with tf.GradientTape() as tape:\n",
    "        outputs_tr = model(inputs_tr, 1)\n",
    "        # Only Nodes\n",
    "        outputs_tr_nodes = outputs_tr[0].nodes\n",
    "        targets_tr_nodes = targets_tr.nodes\n",
    "        # Loss\n",
    "        #print(outputs_tr_nodes)\n",
    "        #print(targets_tr_nodes)\n",
    "        #c_output_tr_nodes, c_target_tr_nodes = remove_effector(outputs_tr_nodes, targets_tr_nodes)\n",
    "        loss_tr = velocity_loss_single_step(targets_tr, outputs_tr[0])\n",
    "    \n",
    "    gradients = tape.gradient(loss_tr, model.trainable_variables)\n",
    "    optimizer.apply(gradients, model.trainable_variables)\n",
    "    return outputs_tr, loss_tr, gradients\n",
    "\n",
    "def test_step(inputs, targets):\n",
    "    outputs = model(inputs, 1)\n",
    "    # Only Nodes\n",
    "    output_nodes = outputs[0].nodes\n",
    "    target_nodes = targets.nodes\n",
    "    # Loss\n",
    "    #c_output_nodes, c_target_nodes = remove_effector(output_nodes, target_nodes)\n",
    "    loss = velocity_loss_single_step(targets, outputs[0])\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "feeder = DataFeeder(SEED)\n",
    "#Get some example data that resembles the tensors that will be fed\n",
    "# into update_step():\n",
    "d = feeder.get_data(dfs = df_dict,\n",
    "                    tr_batch_size = BATCH_SIZE_TR,\n",
    "                    va_batch_size = BATCH_SIZE_GE,\n",
    "                    te_batch_size = BATCH_SIZE_TE,\n",
    "                    graph_creator = rigid_graph_from_pos_closest)\n",
    "\n",
    "X_tr = d[\"X_tr\"]\n",
    "Y_tr = d[\"Y_tr\"]\n",
    "\n",
    "# Get the input signature for that function by obtaining the specs\n",
    "input_signature = [\n",
    "  utils_tf.specs_from_graphs_tuple(X_tr),\n",
    "  utils_tf.specs_from_graphs_tuple(Y_tr)\n",
    "]\n",
    "\n",
    "# Compile the update function using the input signature for speedy code.\n",
    "compiled_update_step = tf.function(update_step, input_signature=input_signature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# (iteration number) out of max iteration, T (elapsed seconds), Ltr, Lte, Lge\n",
      "WARNING:tensorflow:From C:\\Users\\vbenc\\miniconda3\\envs\\Honours\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vbenc\\miniconda3\\envs\\Honours\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:449: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/EncodeProcessDecode/Processor/graph_network/edge_block/broadcast_receiver_nodes_to_edges/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/EncodeProcessDecode/Processor/graph_network/edge_block/broadcast_receiver_nodes_to_edges/Reshape:0\", shape=(None, 1024), dtype=float32), dense_shape=Tensor(\"gradient_tape/EncodeProcessDecode/Processor/graph_network/edge_block/broadcast_receiver_nodes_to_edges/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n",
      "C:\\Users\\vbenc\\miniconda3\\envs\\Honours\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:449: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/EncodeProcessDecode/Processor/graph_network/edge_block/broadcast_sender_nodes_to_edges/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/EncodeProcessDecode/Processor/graph_network/edge_block/broadcast_sender_nodes_to_edges/Reshape:0\", shape=(None, 1024), dtype=float32), dense_shape=Tensor(\"gradient_tape/EncodeProcessDecode/Processor/graph_network/edge_block/broadcast_sender_nodes_to_edges/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 00000, Max 00500, T 11.6, Ltr 1.552497, Lva 82.904869, Lte 82.955223\n",
      "# 00005, Max 00500, T 12.5, Ltr 1.742050, Lva 0.491208, Lte 0.482782\n",
      "# 00010, Max 00500, T 13.5, Ltr 0.187102, Lva 0.027847, Lte 0.027931\n",
      "# 00015, Max 00500, T 14.4, Ltr 0.158059, Lva 0.092871, Lte 0.088417\n",
      "# 00020, Max 00500, T 15.4, Ltr 0.054512, Lva 0.065246, Lte 0.068971\n",
      "# 00025, Max 00500, T 16.3, Ltr 0.004702, Lva 0.017504, Lte 0.015344\n",
      "# 00030, Max 00500, T 17.3, Ltr 0.015499, Lva 0.002630, Lte 0.004649\n",
      "# 00035, Max 00500, T 18.2, Ltr 0.018478, Lva 0.012847, Lte 0.013435\n",
      "# 00040, Max 00500, T 19.2, Ltr 0.009410, Lva 0.012300, Lte 0.011331\n",
      "# 00045, Max 00500, T 20.1, Ltr 0.003015, Lva 0.004268, Lte 0.005869\n",
      "# 00050, Max 00500, T 21.1, Ltr 0.003981, Lva 0.002041, Lte 0.003022\n",
      "# 00055, Max 00500, T 22.1, Ltr 0.003633, Lva 0.002823, Lte 0.003192\n",
      "# 00060, Max 00500, T 23.0, Ltr 0.003645, Lva 0.003243, Lte 0.004078\n",
      "# 00065, Max 00500, T 24.0, Ltr 0.003648, Lva 0.003992, Lte 0.005335\n",
      "# 00070, Max 00500, T 25.0, Ltr 0.003923, Lva 0.002678, Lte 0.003523\n",
      "# 00075, Max 00500, T 25.9, Ltr 0.003443, Lva 0.002029, Lte 0.004032\n",
      "# 00080, Max 00500, T 26.9, Ltr 0.002565, Lva 0.002941, Lte 0.003546\n",
      "# 00085, Max 00500, T 27.9, Ltr 0.002980, Lva 0.003241, Lte 0.003900\n",
      "# 00090, Max 00500, T 28.8, Ltr 0.003366, Lva 0.002314, Lte 0.003569\n",
      "# 00095, Max 00500, T 29.7, Ltr 0.004090, Lva 0.001953, Lte 0.004336\n",
      "# 00100, Max 00500, T 30.7, Ltr 0.003281, Lva 0.002239, Lte 0.002446\n",
      "# 00105, Max 00500, T 31.6, Ltr 0.003460, Lva 0.002538, Lte 0.002085\n",
      "# 00110, Max 00500, T 32.6, Ltr 0.002023, Lva 0.002951, Lte 0.003176\n",
      "# 00115, Max 00500, T 33.5, Ltr 0.003432, Lva 0.003186, Lte 0.002537\n",
      "# 00120, Max 00500, T 34.5, Ltr 0.002880, Lva 0.001748, Lte 0.002902\n",
      "# 00125, Max 00500, T 35.4, Ltr 0.002843, Lva 0.002161, Lte 0.003696\n",
      "# 00130, Max 00500, T 36.4, Ltr 0.002890, Lva 0.002371, Lte 0.005238\n",
      "# 00135, Max 00500, T 37.3, Ltr 0.001661, Lva 0.003871, Lte 0.003132\n",
      "# 00140, Max 00500, T 38.2, Ltr 0.003681, Lva 0.002735, Lte 0.002961\n",
      "# 00145, Max 00500, T 39.2, Ltr 0.004002, Lva 0.002766, Lte 0.003326\n",
      "# 00150, Max 00500, T 40.2, Ltr 0.003218, Lva 0.004167, Lte 0.004591\n",
      "# 00155, Max 00500, T 41.1, Ltr 0.003427, Lva 0.002272, Lte 0.003117\n",
      "# 00160, Max 00500, T 42.1, Ltr 0.003167, Lva 0.003549, Lte 0.005041\n",
      "# 00165, Max 00500, T 43.0, Ltr 0.004803, Lva 0.002594, Lte 0.003913\n",
      "# 00170, Max 00500, T 43.9, Ltr 0.003204, Lva 0.004037, Lte 0.005786\n",
      "# 00175, Max 00500, T 44.9, Ltr 0.002452, Lva 0.003295, Lte 0.003270\n",
      "# 00180, Max 00500, T 45.8, Ltr 0.005165, Lva 0.003294, Lte 0.003466\n",
      "# 00185, Max 00500, T 46.7, Ltr 0.003834, Lva 0.003501, Lte 0.004005\n",
      "# 00190, Max 00500, T 47.7, Ltr 0.002920, Lva 0.001637, Lte 0.002167\n",
      "# 00195, Max 00500, T 48.6, Ltr 0.003858, Lva 0.003090, Lte 0.002663\n",
      "# 00200, Max 00500, T 49.5, Ltr 0.002856, Lva 0.002593, Lte 0.002323\n",
      "# 00205, Max 00500, T 50.5, Ltr 0.002556, Lva 0.002290, Lte 0.002001\n",
      "# 00210, Max 00500, T 51.4, Ltr 0.002951, Lva 0.002540, Lte 0.003397\n",
      "# 00215, Max 00500, T 52.3, Ltr 0.002797, Lva 0.002266, Lte 0.003276\n",
      "# 00220, Max 00500, T 53.3, Ltr 0.003880, Lva 0.002917, Lte 0.002821\n",
      "# 00225, Max 00500, T 54.2, Ltr 0.002703, Lva 0.001945, Lte 0.002291\n",
      "# 00230, Max 00500, T 55.2, Ltr 0.003244, Lva 0.002380, Lte 0.003385\n",
      "# 00235, Max 00500, T 56.2, Ltr 0.002943, Lva 0.003020, Lte 0.002734\n",
      "# 00240, Max 00500, T 57.1, Ltr 0.002878, Lva 0.003234, Lte 0.005013\n",
      "# 00245, Max 00500, T 58.1, Ltr 0.003386, Lva 0.003065, Lte 0.003640\n",
      "# 00250, Max 00500, T 59.1, Ltr 0.003844, Lva 0.003670, Lte 0.003556\n",
      "# 00255, Max 00500, T 60.0, Ltr 0.004542, Lva 0.003055, Lte 0.003854\n",
      "# 00260, Max 00500, T 61.0, Ltr 0.004659, Lva 0.002370, Lte 0.003618\n",
      "# 00265, Max 00500, T 62.0, Ltr 0.003799, Lva 0.002734, Lte 0.002204\n",
      "# 00270, Max 00500, T 63.0, Ltr 0.005486, Lva 0.002359, Lte 0.002594\n",
      "# 00275, Max 00500, T 64.0, Ltr 0.003319, Lva 0.003410, Lte 0.003088\n",
      "# 00280, Max 00500, T 65.0, Ltr 0.003176, Lva 0.003115, Lte 0.003056\n",
      "# 00285, Max 00500, T 65.9, Ltr 0.002723, Lva 0.002739, Lte 0.002937\n",
      "# 00290, Max 00500, T 66.9, Ltr 0.003122, Lva 0.003473, Lte 0.003175\n",
      "# 00295, Max 00500, T 67.8, Ltr 0.003644, Lva 0.003216, Lte 0.003233\n",
      "# 00300, Max 00500, T 68.8, Ltr 0.003996, Lva 0.002567, Lte 0.003879\n"
     ]
    }
   ],
   "source": [
    "# Setup logging\n",
    "last_iteration = 0\n",
    "logged_iterations = []\n",
    "losses_step_tr = []\n",
    "losses_step_va = []\n",
    "losses_step_te = []\n",
    "gradients = []\n",
    "\n",
    "log_every_iteration = 5\n",
    "\n",
    "print(\"# (iteration number) out of max iteration, T (elapsed seconds), \"\n",
    "      \"Ltr, \"\n",
    "      \"Lte, \"\n",
    "      \"Lge\")\n",
    "\n",
    "\n",
    "feeder = DataFeeder(SEED)\n",
    "\n",
    "start_time = time.time()\n",
    "last_log_time = start_time\n",
    "for iteration in range(last_iteration, NUM_TRAINING_ITERATIONS):\n",
    "    last_iteration = iteration\n",
    "    do_logging = (last_iteration % log_every_iteration == 0)\n",
    "    \n",
    "    \n",
    "    data = feeder.get_data(dfs = df_dict, \n",
    "                 tr_batch_size = BATCH_SIZE_TR,\n",
    "                 va_batch_size = BATCH_SIZE_GE,\n",
    "                 te_batch_size = BATCH_SIZE_TE,\n",
    "                 graph_creator = rigid_graph_from_pos_closest, \n",
    "                 te_and_va = do_logging)\n",
    "    \n",
    "    X_tr = data[\"X_tr\"]\n",
    "    Y_tr = data[\"Y_tr\"]\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Do one tick of training    \n",
    "    outputs_tr, loss_tr, gradient = compiled_update_step(X_tr, Y_tr)\n",
    "    \n",
    "    # Don't log at every iteration\n",
    "    if do_logging:\n",
    "        X_va = data[\"X_va\"]\n",
    "        Y_va = data[\"Y_va\"]\n",
    "        X_te = data[\"X_te\"]\n",
    "        Y_te = data[\"Y_te\"]\n",
    "        # Validation and Test\n",
    "        loss_va = test_step(X_va, Y_va)\n",
    "        loss_te = test_step(X_te, Y_te)\n",
    "        \n",
    "        # Convert losses to normal values\n",
    "        loss_tr = loss_tr[0].numpy()\n",
    "        loss_va = loss_va[0].numpy()\n",
    "        loss_te = loss_te[0].numpy()\n",
    "        \n",
    "        the_time = time.time()\n",
    "        elapsed_since_last_log = the_time - last_log_time\n",
    "        last_log_time = the_time\n",
    "        \n",
    "        # Log data into lists\n",
    "        elapsed = time.time() - start_time\n",
    "        losses_step_tr.append(loss_tr)\n",
    "        #if loss_tr > 1.0:\n",
    "        #    print(data[\"tr_indeces\"])\n",
    "        losses_step_va.append(loss_va)\n",
    "        losses_step_te.append(loss_te)\n",
    "        gradients.append(gradient)\n",
    "        \n",
    "        logged_iterations.append(iteration)\n",
    "        \n",
    "        print(\"# {:05d}, Max {:05d}, T {:.1f}, Ltr {:.6f}, Lva {:.6f}, Lte {:.6f}\".format(\n",
    "            iteration, NUM_TRAINING_ITERATIONS, elapsed, loss_tr, loss_va, loss_te))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(name, data):\n",
    "    data = np.array(data)\n",
    "    if os.path.exists(os.path.join(path_saves, name)):\n",
    "        print(\"The file: \"+ name + \" already exists. Delete it before saving a new trajectory!\")\n",
    "    else:\n",
    "        if not os.path.exists(os.path.join(path_saves)):\n",
    "            os.makedirs(os.path.join(path_saves)) \n",
    "            \n",
    "        np.save(os.path.join(path_saves, name), data)\n",
    "\n",
    "save_data(\"iterations\", logged_iterations)\n",
    "save_data(\"losses_tr\", losses_step_tr)\n",
    "save_data(\"losses_te\", losses_step_te)\n",
    "save_data(\"losses_va\", losses_step_va)\n",
    "save_data(\"gradients\", gradients)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "fig, ax = training_plot(logged_iterations, losses_step_tr, losses_step_va, losses_step_te, path_plots)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "fig, ax = training_plot_log_scale(logged_iterations, losses_step_tr, losses_step_va, losses_step_te, path_plots)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for the visualisation \n",
    "ROLLOUT_BATCH_SIZE = 1\n",
    "ROLLOUT_TIMESTEPS = 5\n",
    "\n",
    "sim = PushDatasetSimulator(rollout_steps = ROLLOUT_TIMESTEPS, \n",
    "                           step_size = STEP_SIZE,\n",
    "                           batch_size = ROLLOUT_BATCH_SIZE)\n",
    "\n",
    "# Get trajectory from the dataset\n",
    "X_g, Y_g, i_np = sim.get_trajectories(df_te_1, df_te_2, rigid_graph_from_pos_closest)\n",
    "\n",
    "# Convert trajectories into graphs\n",
    "traj_X, traj_Y = sim.convert_trajectories(X_g,Y_g)\n",
    "# Predict trajectory using the model\n",
    "pred_trajectory, real_trajectory = sim.predict_trajectory_velocity(model, traj_X, traj_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "fig, ax, anim = visualise_trajectory(pred_trajectory, real_trajectory, ROLLOUT_TIMESTEPS)\n",
    "anim.save(os.path.join(path_animations,'visualisation.gif'), writer='imagemagick', fps=60)\n",
    "plt.close('all')\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
